services:

    devcontainer:
        container_name: devcontainer
        build:
          context: ..
          dockerfile: docker/Dockerfile.dev_env_light
        ports:
          - "22023:22"      # SSH
          - "8888:8888"     # Jupyter
          - "4040:4040"     # Spark UI
          - "4041:4041"
          - "4042:4042"
        volumes:
          - ../workspace/sp500:/project/workspace/sp500          
          - ../.git:/project/workspace/sp500/.git
          - ../.gitignore:/project/workspace/sp500/.gitignore          
        working_dir: /project/workspace/sp500
        env_file:
          - ../.env
        environment:
          AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
          AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
          AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}          


    
    zookeeper:
        image: wurstmeister/zookeeper:latest
        container_name: zookeeper_alt
        ports:
          - "2182:2181"
          
          
    course-kafka:
        container_name: course-kafka_alt
        image: wurstmeister/kafka:2.13-2.8.1
        environment:
            KAFKA_ADVERTISED_HOST_NAME: course-kafka
            KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
        ports:
            - "9092:9092"
        depends_on:
            - zookeeper    
    kafdrop:
        container_name: kafdrop_alt
        image: obsidiandynamics/kafdrop:3.30.0
        ports:
            - "9003:9000"
        environment:
            - KAFKA_BROKERCONNECT=course-kafka:9092
        depends_on:
            - course-kafka
    kafka:
        image: wurstmeister/kafka:2.13-2.8.1
        container_name: kafka_alt
        ports:
          - "9093:9092"
        environment:
          KAFKA_ADVERTISED_HOST_NAME: kafka
          KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
        depends_on:
          - zookeeper

    postgres:
        image: postgres:15
        container_name: postgres_alt
        environment:
          - POSTGRES_USER=postgres
          - POSTGRES_PASSWORD=postgres
          - POSTGRES_DB=airflow
        ports:
          - "5433:5432"

    airflow-init:
        image: apache/airflow:2.8.1
        container_name: airflow_init_alt
        depends_on:
          - postgres
        environment:
          - AIRFLOW__CORE__EXECUTOR=LocalExecutor
          - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/airflow
          - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
          - AIRFLOW__CORE__LOAD_EXAMPLES=False
        volumes:
          - ../workspace/sp500/dags:/opt/airflow/dags
        entrypoint: /bin/bash
        command:
          - -c
          - >
            airflow users list || (
              airflow db init &&
              airflow users create
                --role Admin
                --username airflow
                --password airflow
                --email airflow@airflow.com
                --firstname airflow
                --lastname airflow
            )
        restart: on-failure

    airflow-webserver:
        image: apache/airflow:2.8.1
        container_name: airflow_webserver_alt
        depends_on:
          - postgres
        environment:
          - AIRFLOW__CORE__EXECUTOR=LocalExecutor
          - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/airflow
          - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
          - AIRFLOW__CORE__LOAD_EXAMPLES=False
        volumes:
          - ../workspace/sp500/dags:/opt/airflow/dags
        ports:
          - "8083:8080"
        command: airflow webserver
        restart: always

    airflow-scheduler:
        image: apache/airflow:2.8.1
        container_name: airflow_scheduler_alt
        depends_on:
          - postgres
        environment:
          - AIRFLOW__CORE__EXECUTOR=LocalExecutor
          - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/airflow
          - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
          - AIRFLOW__CORE__LOAD_EXAMPLES=False
        volumes:
          - ../workspace/sp500/dags:/opt/airflow/dags
        command: airflow scheduler
        restart: always

    kafka-consumer:
        build:
          context: ..
          dockerfile: docker/Dockerfile.consumer
        container_name: kafka_consumer_alt
        command: python /app/main_consumer.py
        restart: always
        depends_on:
          - kafka
        env_file:
          - ../.env
        volumes:
          - ../workspace/sp500:/project/workspace/sp500

    kafka-producer:
        build:
          context: ..
          dockerfile: docker/Dockerfile.producer
        container_name: kafka_producer_alt
        command: python /app/main_producer.py
        depends_on:
          - kafka 
        volumes:
          - ../workspace/sp500:/project/workspace/sp500
        env_file:
          - ../.env
        environment:
          AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
          AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
          AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}    

